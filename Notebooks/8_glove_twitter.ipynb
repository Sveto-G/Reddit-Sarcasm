{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be5a6e0e-4ca7-4aa1-8857-92406ba28df3",
   "metadata": {},
   "source": [
    "### Notebook #8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afd7bf4-f555-4b12-be99-5d5d6618af0a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Table of Contents\n",
    "1) <a href='#eda' id='top'>Glove Twitter</a>\n",
    "2) <a href='#mod'>Model</a>\n",
    "3) <a href='#fin'>Conclusion</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaecc535-f890-4ded-b0ec-990db9a48fbd",
   "metadata": {},
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ad65ef-59a5-4f32-b3f5-134104979526",
   "metadata": {},
   "source": [
    "This notebook endeavours to divine whether pre-trained word embeddings will outperform the ones trained on the dataset itself. The embeddings we have chosen are the `Glove-Twitter-200'. The rationale behind choosing these particular pre-trained embeddings is that the language and tone between Reddit and Twitter are more in line with one another than Reddit is with Wikipedia. This similarity, along with the more robust training and vocabulary that these embeddings possess may thereby lead to different, if not better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fad59733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import regex as re\n",
    "import string\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(action = 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "575d6a6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1010714, 2)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading in csv and verifying shape\n",
    "reddit = pd.read_csv('reddit_comments.csv', index_col=0)\n",
    "\n",
    "reddit.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359f5940-4ce8-4d41-8559-83d2cd74628a",
   "metadata": {},
   "source": [
    "## <a href='#top' id='eda'>Glove Twitter</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1559d932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading in twitter embeddings\n",
    "embeddings = api.load('glove-twitter-200')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "75403910-fd61-4cac-9c3b-ba79291565b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glove Twitter consists of 1193514 tokens and 200 dimensional vectors\n"
     ]
    }
   ],
   "source": [
    "# verifying and printing shape\n",
    "shape = embeddings.vectors.shape\n",
    "\n",
    "print(f'Glove Twitter consists of {shape[0]} tokens and {shape[1]} dimensional vectors')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76aaf154-ffd8-41b4-ba94-8b51f1853109",
   "metadata": {},
   "source": [
    "**Below we will take a look at the same group of words we have examined using our self-trained embeddings to see if we can spot any key differences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "98f5d1b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('witty', 0.6640589237213135),\n",
       " ('bitchy', 0.6119109392166138),\n",
       " ('sarcasm', 0.6098357439041138),\n",
       " ('rude', 0.5943841934204102),\n",
       " ('insensitive', 0.5871057510375977),\n",
       " ('condescending', 0.5716186165809631),\n",
       " ('snarky', 0.5695698261260986),\n",
       " ('clever', 0.5692526698112488),\n",
       " ('sassy', 0.567287802696228),\n",
       " ('smartass', 0.5572746992111206)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.most_similar('sarcastic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e51a6e7-4b3a-4ca8-bc8e-ca3fd2b82e9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('intentions', 0.6637458801269531),\n",
       " ('actions', 0.5589097142219543),\n",
       " ('intent', 0.5574334263801575),\n",
       " ('purpose', 0.5178261399269104),\n",
       " ('without', 0.4837496280670166),\n",
       " ('importance', 0.4837445616722107),\n",
       " ('intend', 0.4802425801753998),\n",
       " ('ambition', 0.4783967435359955),\n",
       " ('capable', 0.47621142864227295),\n",
       " ('consequence', 0.4741327464580536)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.most_similar('intention')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9174c0f6-bae8-429f-aad7-f8d1cf4fe0fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('misled', 0.5900747776031494),\n",
       " ('deceive', 0.552757978439331),\n",
       " ('manipulate', 0.551977276802063),\n",
       " ('manipulated', 0.519615650177002),\n",
       " ('deceived', 0.5073312520980835),\n",
       " ('tricked', 0.49346911907196045),\n",
       " ('naive', 0.4644486606121063),\n",
       " ('portray', 0.45692503452301025),\n",
       " ('misinformed', 0.45582106709480286),\n",
       " ('brainwashed', 0.45131373405456543)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.most_similar('mislead')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3128675e-e292-4c24-9974-90d0748ff2bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('authentic', 0.5544952154159546),\n",
       " ('sincere', 0.5459144115447998),\n",
       " ('unique', 0.5418863296508789),\n",
       " ('silver', 0.5259337425231934),\n",
       " ('leather', 0.5243769884109497),\n",
       " ('honest', 0.518162727355957),\n",
       " ('quality', 0.5110219717025757),\n",
       " ('solid', 0.5067755579948425),\n",
       " ('genuinely', 0.50606369972229),\n",
       " ('truly', 0.5023698806762695)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.most_similar('genuine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5ca61347",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('truthful', 0.691604733467102),\n",
       " ('rather', 0.672570526599884),\n",
       " ('loyal', 0.6538525819778442),\n",
       " ('honestly', 0.6515498161315918),\n",
       " ('honesty', 0.6430395841598511),\n",
       " ('lie', 0.6378211975097656),\n",
       " ('admit', 0.6351155638694763),\n",
       " ('being', 0.6288948059082031),\n",
       " ('person', 0.6275302171707153),\n",
       " ('true', 0.6265100836753845)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.most_similar('honest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d5eb0633-3797-4ddb-bd8b-62b58c273270",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lie</th>\n",
       "      <th>truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lies</td>\n",
       "      <td>lie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tell</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lying</td>\n",
       "      <td>lies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>truth</td>\n",
       "      <td>nothing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>n't</td>\n",
       "      <td>tell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>never</td>\n",
       "      <td>know</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>dont</td>\n",
       "      <td>that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>know</td>\n",
       "      <td>but</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>fool</td>\n",
       "      <td>words</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>say</td>\n",
       "      <td>about</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     lie    truth\n",
       "0   lies      lie\n",
       "1   tell     true\n",
       "2  lying     lies\n",
       "3  truth  nothing\n",
       "4    n't     tell\n",
       "5  never     know\n",
       "6   dont     that\n",
       "7   know      but\n",
       "8   fool    words\n",
       "9    say    about"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finding most similar words to 'truth' and 'lie'\n",
    "lie = embeddings.most_similar('lie')\n",
    "truth = embeddings.most_similar('truth')\n",
    "\n",
    "# setting to dataframe to visualize\n",
    "pd.DataFrame(\n",
    "        data={\"lie\": [word for word, sim in lie], \n",
    "            \"truth\": [word for word, sim in truth]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aba0ecae-c9fd-41de-a8b3-29a7131a93fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>serious</th>\n",
       "      <th>joking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>seriously</td>\n",
       "      <td>kidding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>really</td>\n",
       "      <td>obviously</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>but</td>\n",
       "      <td>laughing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>still</td>\n",
       "      <td>joke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>funny</td>\n",
       "      <td>lololol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>talk</td>\n",
       "      <td>obv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>actually</td>\n",
       "      <td>lolol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>damn</td>\n",
       "      <td>kiddin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>something</td>\n",
       "      <td>jokin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>thats</td>\n",
       "      <td>seriously</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     serious     joking\n",
       "0  seriously    kidding\n",
       "1     really  obviously\n",
       "2        but   laughing\n",
       "3      still       joke\n",
       "4      funny    lololol\n",
       "5       talk        obv\n",
       "6   actually      lolol\n",
       "7       damn     kiddin\n",
       "8  something      jokin\n",
       "9      thats  seriously"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similarity scores for 'serious' and 'joking'\n",
    "serious = embeddings.most_similar('serious')\n",
    "joking = embeddings.most_similar('joking')\n",
    "\n",
    "# setting to dataframe\n",
    "pd.DataFrame(\n",
    "        data={\"serious\": [word for word, sim in serious], \n",
    "            \"joking\": [word for word, sim in joking]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcfad2c-ede4-437d-acea-affdeec485dc",
   "metadata": {},
   "source": [
    "### Summary of differences\n",
    "- 'sarcastic' nowhere to be found in 'serious' and 'joking' top 10\n",
    "- abbreviations and acronyms\n",
    "- more repetition: i.e. lie -> lies -> lying\n",
    "- interestingly the most related word to truth is lie\n",
    "- results are for the most part different than the pre-trained, but a similar pattern persists:\n",
    "  i) words are either related or synonyms\n",
    "  ii) grammatically equivalent\n",
    "  iii) antonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1101cf60-31b3-4945-a813-584ba9fc01ef",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d63fbe-46d8-44b4-ae31-e51a044b4a8b",
   "metadata": {},
   "source": [
    "Below we will instantiate the tokenizer as well as the vectorizer function, set our X and y, and perfrom train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "168c0445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom tokenizer function\n",
    "\n",
    "def tokenizer(document):\n",
    "    \n",
    "    \n",
    "    #removing punctuation\n",
    "    for punc in string.punctuation:\n",
    "        document = document.replace(punc, '')\n",
    "        # removing numbers and setting all documents to lowercase    \n",
    "    document = re.sub(\"\\d+\", \"\", document).lower()\n",
    "        # splitting documents and appending tokens list\n",
    "    tokens = document.split(' ')\n",
    "        \n",
    "    \n",
    "        \n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ecefaed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizer(document):\n",
    "\n",
    "    tokens = tokenizer(document) # calling tokenizer function\n",
    "    \n",
    "    size = embeddings.vector_size # setting constant document size\n",
    "    vec_doc = np.zeros(size) # populating list of document size with zeros \n",
    "    count = 1 # word count to be used for average\n",
    "    \n",
    "    # looping through documents\n",
    "    for word in tokens:\n",
    "        # checking if word exists in document\n",
    "        if word in embeddings:\n",
    "            count +=1\n",
    "            vec_doc += embeddings[word] # adding word embedding to doc embedding\n",
    "    \n",
    "    vec_doc = vec_doc / count # taking average of embeddings\n",
    "    \n",
    "    \n",
    "    return vec_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "98022010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying vectorizer function to comments\n",
    "reddit['vectors'] = reddit['comment'].apply(vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b54f0794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating X and y\n",
    "X = reddit['vectors']\n",
    "y = reddit['label']\n",
    "\n",
    "X = list(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "62ec84a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test split\n",
    "X_rem, X_test, y_rem, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_rem, y_rem, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d8986b-9aee-4068-88d5-2e6055e93c02",
   "metadata": {},
   "source": [
    "## <a href='#top' id='mod'>Model</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d9c75685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.62623999\n",
      "Validation score: 0.662674\n",
      "Iteration 2, loss = 0.60506743\n",
      "Validation score: 0.670308\n",
      "Iteration 3, loss = 0.59702490\n",
      "Validation score: 0.673720\n",
      "Iteration 4, loss = 0.59140491\n",
      "Validation score: 0.676745\n",
      "Iteration 5, loss = 0.58761974\n",
      "Validation score: 0.678381\n",
      "Iteration 6, loss = 0.58442868\n",
      "Validation score: 0.678944\n",
      "Iteration 7, loss = 0.58177573\n",
      "Validation score: 0.677449\n",
      "Iteration 8, loss = 0.57942868\n",
      "Validation score: 0.679823\n",
      "Iteration 9, loss = 0.57764762\n",
      "Validation score: 0.680949\n",
      "Iteration 10, loss = 0.57602169\n",
      "Validation score: 0.680421\n",
      "Iteration 11, loss = 0.57410248\n",
      "Validation score: 0.677695\n",
      "Iteration 12, loss = 0.57274648\n",
      "Validation score: 0.680967\n",
      "Iteration 13, loss = 0.57145793\n",
      "Validation score: 0.679419\n",
      "Iteration 14, loss = 0.57031113\n",
      "Validation score: 0.683341\n",
      "Iteration 15, loss = 0.56916527\n",
      "Validation score: 0.681705\n",
      "Iteration 16, loss = 0.56786190\n",
      "Validation score: 0.679648\n",
      "Iteration 17, loss = 0.56702685\n",
      "Validation score: 0.680369\n",
      "Iteration 18, loss = 0.56619814\n",
      "Validation score: 0.676306\n",
      "Iteration 19, loss = 0.56546536\n",
      "Validation score: 0.680580\n",
      "Iteration 20, loss = 0.56452319\n",
      "Validation score: 0.678451\n",
      "Iteration 21, loss = 0.56422937\n",
      "Validation score: 0.679261\n",
      "Iteration 22, loss = 0.56304712\n",
      "Validation score: 0.680492\n",
      "Iteration 23, loss = 0.56292944\n",
      "Validation score: 0.683306\n",
      "Iteration 24, loss = 0.56224913\n",
      "Validation score: 0.677255\n",
      "Iteration 25, loss = 0.56157733\n",
      "Validation score: 0.680122\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(early_stopping=True, max_iter=50, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(early_stopping=True, max_iter=50, verbose=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(early_stopping=True, max_iter=50, verbose=True)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random seed\n",
    "np.random.seed(42)\n",
    "# mlp classifier instatiation\n",
    "MLP = MLPClassifier(verbose=True, max_iter=50, early_stopping=True)\n",
    "# fitting to train\n",
    "MLP.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "912fcc84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.7029212384306083\n",
      "Val Score: 0.6818145840039259\n"
     ]
    }
   ],
   "source": [
    "print(f'Train Score: {MLP.score(X_train, y_train)}\\nVal Score: {MLP.score(X_val, y_val)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8a0644-df34-4f7a-a1a9-3b6fd5b33eef",
   "metadata": {},
   "source": [
    "**The results are fairly similar to, but lower than the self-trained model. The self-trained model outperformed this by 2% in both train and validation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d8bdc7-2fe0-4832-b0aa-a1e65089dcfa",
   "metadata": {},
   "source": [
    "Let us take a quick look at the precision and recall scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d48c0c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non-Sarcastic       0.68      0.70      0.69     94698\n",
      "    Sarcastic       0.69      0.66      0.68     94811\n",
      "\n",
      "     accuracy                           0.68    189509\n",
      "    macro avg       0.68      0.68      0.68    189509\n",
      " weighted avg       0.68      0.68      0.68    189509\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = MLP.predict(X_val)\n",
    "report = classification_report(y_val, predictions, target_names=['Non-Sarcastic', 'Sarcastic'])\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c434679d-d858-4d30-880d-f3f5ec482f0e",
   "metadata": {},
   "source": [
    "Rather predictably, the precision and recall scores hover around +-4% from the accuracy score and do not deviate much from one another - neither in the sarcastic or non-sarcastic sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60e2e76-dcf9-4ac1-a019-f888c19d5ae6",
   "metadata": {},
   "source": [
    "## <a href='#top' id='fin'>Conclusion</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6d25d3-2c22-4c32-a567-9618a6527c9e",
   "metadata": {},
   "source": [
    "All in all the `Glove-Twitter` embeddings performed quite similarily to the self-trained embeddings. The difference is slim, but potentially significant, as every decimal point counts to an optimized solution. We could see some differences in the word similarities between these two embeddings, but it is hard to say what that means for their predictive values. In the final notebook we will be using another set of pre-trained embeddings in `Word2Vec Google News`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sarcasm_v3",
   "language": "python",
   "name": "sarcasm_v3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
