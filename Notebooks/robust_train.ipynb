{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb1ae7c5-83b9-4744-a459-f1036c985559",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gravi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import regex as re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(action = 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f91991c-8706-4844-b142-f031169af10e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NC and NH.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>You do know west teams play against west teams...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>They were underdogs earlier today, but since G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>This meme isn't funny none of the \"new york ni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>I could use one of those tools.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment\n",
       "0      0                                         NC and NH.\n",
       "1      0  You do know west teams play against west teams...\n",
       "2      0  They were underdogs earlier today, but since G...\n",
       "3      0  This meme isn't funny none of the \"new york ni...\n",
       "4      0                    I could use one of those tools."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit = pd.read_csv('reddit_comments.csv', index_col=0)\n",
    "\n",
    "reddit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b5e8b1e-f35f-4846-aca1-ba7941e00d5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1010714, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfae09e4-1d52-4395-a092-ab25605ecadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom tokenizer function\n",
    "\n",
    "def tokenizer(series):\n",
    "    # get values from series\n",
    "    corpus = series.values\n",
    "    tokens = [] # empty token list\n",
    "    \n",
    "    for document in corpus:\n",
    "        #removing punctuation\n",
    "        for punc in string.punctuation:\n",
    "            document = document.replace(punc, '')\n",
    "        # removing numbers and setting all documents to lowercase    \n",
    "        document = re.sub(\"\\d+\", \"\", document).lower()\n",
    "        # splitting documents and appending tokens list\n",
    "        tokens.append(document.split(' '))\n",
    "        \n",
    "    \n",
    "        \n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73e3ea6a-f2ee-4cb7-85b9-f358d24d4686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting X and y\n",
    "X = reddit['comment']\n",
    "y = reddit['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa417af5-e200-47dd-a81a-6aff53d1492e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing our comments\n",
    "tokenized_data = tokenizer(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01019f49-dd6c-47e9-9ae4-0bf94a7bbd63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32246745, 52702745)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reinstantiating more precise word2vec \n",
    "model = Word2Vec(tokenized_data, window=5, min_count=25, workers=4, size=300, max_vocab_size=10000 )\n",
    "model.train(tokenized_data, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c38cce90-39fd-484d-aeec-58c7bff1768f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence(document):\n",
    "    size = model.vector_size\n",
    "    word_vec_document = np.zeros(size)\n",
    "    count = 1\n",
    "    \n",
    "    for word in document:\n",
    "        if word in model:\n",
    "            count +=1\n",
    "            word_vec_document += model[word]\n",
    "    \n",
    "    word_vec_document = word_vec_document / count\n",
    "    \n",
    "    return word_vec_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6579144f-b2ed-454f-a860-195e6ad50475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting vectors to variable\n",
    "document_vectors = [sentence(doc) for doc in tokenized_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce77080c-ee9a-4440-8c9b-2f75e37d39cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_samples = pd.read_csv('sample_comments.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ecc6287-da51-45a0-bb73-631ada1b51e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_samp = reddit_samples['comment']\n",
    "y_samp = reddit_samples['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9666a51c-1f99-443c-b67d-0c4d7683b555",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tokens = tokenizer(X_samp)\n",
    "sample_vectors = [sentence(doc) for doc in sample_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "28a643af-8c3f-4612-933e-86529fc1a824",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_samp = sample_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6bbc82c4-9c8d-4c2b-ac37-50524090de3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# initial split into rem and test\n",
    "X_rem, X_test, y_rem, y_test = train_test_split(X_samp, y_samp, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e0b2b01e-27b9-4927-82c4-b361252ad6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# secondary split into train and val\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_rem, y_rem, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2213d4cf-2b3f-4ce2-9ebf-76b5ccb70a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f34d1d22-1baa-4966-931c-29ac04a6f8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_network = MLPClassifier(verbose=True, max_iter=100, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7af9e3a6-0070-4cf0-a4e4-7e56b9ac1f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.59270900\n",
      "Iteration 2, loss = 0.57508077\n",
      "Iteration 3, loss = 0.56878644\n",
      "Iteration 4, loss = 0.56474797\n",
      "Iteration 5, loss = 0.56199006\n",
      "Iteration 6, loss = 0.55965116\n",
      "Iteration 7, loss = 0.55777380\n",
      "Iteration 8, loss = 0.55630830\n",
      "Iteration 9, loss = 0.55489859\n",
      "Iteration 10, loss = 0.55362951\n",
      "Iteration 11, loss = 0.55265670\n",
      "Iteration 12, loss = 0.55159636\n",
      "Iteration 13, loss = 0.55079818\n",
      "Iteration 14, loss = 0.54998870\n",
      "Iteration 15, loss = 0.54924019\n",
      "Iteration 16, loss = 0.54843314\n",
      "Iteration 17, loss = 0.54782395\n",
      "Iteration 18, loss = 0.54719014\n",
      "Iteration 19, loss = 0.54660270\n",
      "Iteration 20, loss = 0.54616194\n",
      "Iteration 21, loss = 0.54571542\n",
      "Iteration 22, loss = 0.54524178\n",
      "Iteration 23, loss = 0.54491333\n",
      "Iteration 24, loss = 0.54450961\n",
      "Iteration 25, loss = 0.54401926\n",
      "Iteration 26, loss = 0.54374856\n",
      "Iteration 27, loss = 0.54343988\n",
      "Iteration 28, loss = 0.54310917\n",
      "Iteration 29, loss = 0.54287270\n",
      "Iteration 30, loss = 0.54262736\n",
      "Iteration 31, loss = 0.54240404\n",
      "Iteration 32, loss = 0.54189646\n",
      "Iteration 33, loss = 0.54178603\n",
      "Iteration 34, loss = 0.54161698\n",
      "Iteration 35, loss = 0.54127478\n",
      "Iteration 36, loss = 0.54113243\n",
      "Iteration 37, loss = 0.54080085\n",
      "Iteration 38, loss = 0.54060042\n",
      "Iteration 39, loss = 0.54055990\n",
      "Iteration 40, loss = 0.54046334\n",
      "Iteration 41, loss = 0.54018950\n",
      "Iteration 42, loss = 0.54000223\n",
      "Iteration 43, loss = 0.53990067\n",
      "Iteration 44, loss = 0.53962038\n",
      "Iteration 45, loss = 0.53950171\n",
      "Iteration 46, loss = 0.53935011\n",
      "Iteration 47, loss = 0.53930417\n",
      "Iteration 48, loss = 0.53914783\n",
      "Iteration 49, loss = 0.53895327\n",
      "Iteration 50, loss = 0.53887928\n",
      "Iteration 51, loss = 0.53874524\n",
      "Iteration 52, loss = 0.53861063\n",
      "Iteration 53, loss = 0.53841613\n",
      "Iteration 54, loss = 0.53838679\n",
      "Iteration 55, loss = 0.53815743\n",
      "Iteration 56, loss = 0.53812822\n",
      "Iteration 57, loss = 0.53811250\n",
      "Iteration 58, loss = 0.53789792\n",
      "Iteration 59, loss = 0.53787482\n",
      "Iteration 60, loss = 0.53770262\n",
      "Iteration 61, loss = 0.53765686\n",
      "Iteration 62, loss = 0.53745025\n",
      "Iteration 63, loss = 0.53756215\n",
      "Iteration 64, loss = 0.53737170\n",
      "Iteration 65, loss = 0.53732908\n",
      "Iteration 66, loss = 0.53722327\n",
      "Iteration 67, loss = 0.53714593\n",
      "Iteration 68, loss = 0.53708879\n",
      "Iteration 69, loss = 0.53694608\n",
      "Iteration 70, loss = 0.53686200\n",
      "Iteration 71, loss = 0.53687839\n",
      "Iteration 72, loss = 0.53675207\n",
      "Iteration 73, loss = 0.53659271\n",
      "Iteration 74, loss = 0.53657225\n",
      "Iteration 75, loss = 0.53655845\n",
      "Iteration 76, loss = 0.53647504\n",
      "Iteration 77, loss = 0.53644752\n",
      "Iteration 78, loss = 0.53620394\n",
      "Iteration 79, loss = 0.53625568\n",
      "Iteration 80, loss = 0.53625615\n",
      "Iteration 81, loss = 0.53613253\n",
      "Iteration 82, loss = 0.53605980\n",
      "Iteration 83, loss = 0.53614640\n",
      "Iteration 84, loss = 0.53598379\n",
      "Iteration 85, loss = 0.53593031\n",
      "Iteration 86, loss = 0.53589957\n",
      "Iteration 87, loss = 0.53569139\n",
      "Iteration 88, loss = 0.53574434\n",
      "Iteration 89, loss = 0.53558861\n",
      "Iteration 90, loss = 0.53560681\n",
      "Iteration 91, loss = 0.53559549\n",
      "Iteration 92, loss = 0.53548552\n",
      "Iteration 93, loss = 0.53557816\n",
      "Iteration 94, loss = 0.53548268\n",
      "Iteration 95, loss = 0.53536619\n",
      "Iteration 96, loss = 0.53528035\n",
      "Iteration 97, loss = 0.53523393\n",
      "Iteration 98, loss = 0.53512423\n",
      "Iteration 99, loss = 0.53525210\n",
      "Iteration 100, loss = 0.53506650\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(max_iter=100, random_state=42, verbose=True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_network.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6c76543b-e6a8-48f3-87d3-68c310de892a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.735342747183936\n",
      "Val: 0.7309510743383004\n"
     ]
    }
   ],
   "source": [
    "print(f'Train: {neural_network.score(X_train, y_train)}')\n",
    "print(f'Val: {neural_network.score(X_val, y_val)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7dd41f09-c730-4173-994d-498544359fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_probas = neural_network.predict_proba(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d16ebb2d-47eb-4c2a-ae2f-1992bc35b68e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.03606707, 0.96393293],\n",
       "       [0.2862299 , 0.7137701 ],\n",
       "       [0.11738899, 0.88261101],\n",
       "       [0.08163301, 0.91836699],\n",
       "       [0.03943854, 0.96056146]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_probas[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6aeffc0d-2912-4f70-bb8a-ccda85929d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bd1e1efa-0b6a-421c-948d-a698f05639a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_probas = val_probas[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "33169436-8840-423f-8692-224d4871e6cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03606707, 0.2862299 , 0.11738899, 0.08163301, 0.03943854])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_probas[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "116d2af5-0cd2-45fe-8885-fd1559ccaf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_predict = neural_network.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0af71b6d-98f2-4322-9cd1-51ffd7bb9bd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_predict[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "90a73822-1ff4-4089-b242-1c9de0ecd22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc_score = accuracy_score(y_val, val_predict)\n",
    "# precision = precision_score(y_val, val_predict)\n",
    "# recall = recall_score(y_val, val_predict)\n",
    "# f1 = f1_score(y_val, val_predict)\n",
    "\n",
    "# print('VALIDATION SUMMARY: \\n')\n",
    "# print(f'Accuracy: {round(acc_score, 2)} \\n Precision: {round(precision, 2)} \\n Recall: {round(recall,2)}  \\n F1: {round(f1, 2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c138fb45-f941-4b85-adc6-96cb9f839267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non-Sarcastic       0.72      0.76      0.74     23647\n",
      "    Sarcastic       0.75      0.70      0.72     23731\n",
      "\n",
      "     accuracy                           0.73     47378\n",
      "    macro avg       0.73      0.73      0.73     47378\n",
      " weighted avg       0.73      0.73      0.73     47378\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_val, val_predict, target_names=['Non-Sarcastic', 'Sarcastic'])\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f9ec195a-c411-48d9-a490-f7feb9e7a281",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7f9823-6632-4632-8a11-49d0c72b511b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
