{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5adf175-5e9e-4572-8db6-15cc1252e4b3",
   "metadata": {},
   "source": [
    "## Comment + Parent Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cfffec-a9f5-41df-b092-efb563a1fc14",
   "metadata": {},
   "source": [
    "### Notebook #4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb24c1bc-5d44-4617-9bb8-34ef2b87baeb",
   "metadata": {},
   "source": [
    "## Intro\n",
    "\n",
    "What follows is a baseline `Logistic Regression` on the reddit `comments` and `parent comments`. We will be using `ColumnTransform` to run `CountVectorizer` on the two columns in parallel. We will be splitting the features 750/750 for a total of 1500 to keep the amount of features consistent. This is an experiment conducted in the hopes of improving the model given added context from the `parent comment`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8d61b83",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xe but this version of numpy is 0xd",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;31mRuntimeError\u001b[0m: module compiled against API version 0xe but this version of numpy is 0xd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gravi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "pd.set_option('display.max_columns', None, 'max_colwidth', 250)\n",
    "\n",
    "import regex as re\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "091c382e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gravi\\anaconda3\\envs\\sarcasm\\lib\\site-packages\\numpy\\lib\\arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>parent_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NC and NH.</td>\n",
       "      <td>Yeah, I get that argument. At this point, I'd prefer is she lived in NC as well.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>You do know west teams play against west teams more than east teams right?</td>\n",
       "      <td>The blazers and Mavericks (The wests 5 and 6 seed) did not even carry a good enough record to make the playoffs in the east last year.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>They were underdogs earlier today, but since Gronk's announcement this afternoon, the Vegas line has moved to patriots -1</td>\n",
       "      <td>They're favored to win.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>This meme isn't funny none of the \"new york nigga\" ones are.</td>\n",
       "      <td>deadass don't kill my buzz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>I could use one of those tools.</td>\n",
       "      <td>Yep can confirm I saw the tool they use for that. It was made by our boy EASports_MUT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  \\\n",
       "0      0   \n",
       "1      0   \n",
       "2      0   \n",
       "3      0   \n",
       "4      0   \n",
       "\n",
       "                                                                                                                     comment  \\\n",
       "0                                                                                                                 NC and NH.   \n",
       "1                                                 You do know west teams play against west teams more than east teams right?   \n",
       "2  They were underdogs earlier today, but since Gronk's announcement this afternoon, the Vegas line has moved to patriots -1   \n",
       "3                                                               This meme isn't funny none of the \"new york nigga\" ones are.   \n",
       "4                                                                                            I could use one of those tools.   \n",
       "\n",
       "                                                                                                                           parent_comment  \n",
       "0                                                        Yeah, I get that argument. At this point, I'd prefer is she lived in NC as well.  \n",
       "1  The blazers and Mavericks (The wests 5 and 6 seed) did not even carry a good enough record to make the playoffs in the east last year.  \n",
       "2                                                                                                                 They're favored to win.  \n",
       "3                                                                                                              deadass don't kill my buzz  \n",
       "4                                                   Yep can confirm I saw the tool they use for that. It was made by our boy EASports_MUT  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading csv and checking head\n",
    "reddit = pd.read_csv('comment_plus_parent.csv', index_col=0)\n",
    "reddit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c7e8ce6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1010714, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verifying shape\n",
    "reddit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6197839-fe03-491b-adab-2b5b7ee8fa6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "252678.5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dividing rows by 4 to create sample\n",
    "reddit.shape[0]/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1772e82-f026-46c7-9437-770f588efcb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>parent_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>139485</th>\n",
       "      <td>0</td>\n",
       "      <td>Trufant</td>\n",
       "      <td>Yo I missed a large portion of the game. Any significant injuries?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276174</th>\n",
       "      <td>0</td>\n",
       "      <td>Too soon</td>\n",
       "      <td>When/If The Bismarck Is Added This Will Be The Scariest Thing a German Player Can See</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590507</th>\n",
       "      <td>0</td>\n",
       "      <td>Also, phone lights have no throw.</td>\n",
       "      <td>No. One of the benefits/requirements of a flashlight is that most edc-size ones can be held in the mouth for hands-free use. Try that with your phone.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>652534</th>\n",
       "      <td>1</td>\n",
       "      <td>The Pirate Bay, of course!</td>\n",
       "      <td>Where is the best place to buy windows? It doesn't matter what version as I plan on upgrading to windows 10. Thank you for your time.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333855</th>\n",
       "      <td>0</td>\n",
       "      <td>Doesn't Sop have a Mac version though?</td>\n",
       "      <td>Right, guess I should say that my work uses Mac's so I can't do acestream at work. I do acestream at home.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        label                                 comment  \\\n",
       "139485      0                                 Trufant   \n",
       "276174      0                                Too soon   \n",
       "590507      0       Also, phone lights have no throw.   \n",
       "652534      1              The Pirate Bay, of course!   \n",
       "333855      0  Doesn't Sop have a Mac version though?   \n",
       "\n",
       "                                                                                                                                                parent_comment  \n",
       "139485                                                                                      Yo I missed a large portion of the game. Any significant injuries?  \n",
       "276174                                                                   When/If The Bismarck Is Added This Will Be The Scariest Thing a German Player Can See  \n",
       "590507  No. One of the benefits/requirements of a flashlight is that most edc-size ones can be held in the mouth for hands-free use. Try that with your phone.  \n",
       "652534                   Where is the best place to buy windows? It doesn't matter what version as I plan on upgrading to windows 10. Thank you for your time.  \n",
       "333855                                              Right, guess I should say that my work uses Mac's so I can't do acestream at work. I do acestream at home.  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating sample to be used for testing\n",
    "reddit_sample = reddit.sample(252679, random_state=42)\n",
    "reddit_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb4f4b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting X and y\n",
    "y = reddit_sample.pop('label')\n",
    "X = reddit_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18890b56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(252679, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verifying X shape\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c261144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(252679,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verifying y shape\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d025c0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first split into test and remainder\n",
    "X_remainder, X_test, y_remainder, y_test = train_test_split(X, y, test_size = 0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07815340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# second split into train and validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_remainder, y_remainder, test_size = 0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd2a1665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom tokenizer function\n",
    "\n",
    "# instantiating stemmer and stopwords\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def tokenizer(sentence):\n",
    "\n",
    "    # replacing numbers with empty string\n",
    "    sentence = re.sub(\"\\d+\", \"\", sentence)\n",
    "    \n",
    "    # removing punctuation and setting to lower case\n",
    "    for punctuation_mark in string.punctuation:\n",
    "        sentence = sentence.replace(punctuation_mark,'').lower()\n",
    "        \n",
    "    \n",
    "    # splitting sentence into words\n",
    "    words_list = sentence.split(' ')\n",
    "    stemmed_words = []\n",
    "    \n",
    "    \n",
    "    # removing stopwords and any tokens that are just empty strings\n",
    "    for word in words_list:\n",
    "        if (not word in stop_words) and (word!=''):\n",
    "            # Stem words\n",
    "            stemmed_word = stemmer.stem(word)\n",
    "            stemmed_words.append(stemmed_word)\n",
    "\n",
    "    return stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4afef237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating transformers to pass into column transformer\n",
    "transformers = [('comment', CountVectorizer(min_df=25, tokenizer=tokenizer, max_features=750, dtype=np.int8), 'comment'),\n",
    "                 ('parent', CountVectorizer(min_df=25, tokenizer=tokenizer, max_features=750, dtype=np.int8), 'parent_comment')]\n",
    "                \n",
    "\n",
    "# Creating the column transformer\n",
    "column_transform = ColumnTransformer(transformers)\n",
    "\n",
    "# Fitting and transforming train, test, val\n",
    "X_train_tokens = column_transform.fit_transform(X_train)\n",
    "X_val_tokens = column_transform.transform(X_val)\n",
    "X_test_tokens = column_transform.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbbdb5e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(142131, 1500)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train to df\n",
    "train_vectors = pd.DataFrame(columns=column_transform.get_feature_names_out(), data=X_train_tokens.toarray())\n",
    "train_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2cceb5a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47378, 1500)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# val to df\n",
    "val_vectors = pd.DataFrame(columns=column_transform.get_feature_names_out(), data=X_val_tokens.toarray())\n",
    "val_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78a7f551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63170, 1500)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test to df\n",
    "test_vectors = pd.DataFrame(columns=column_transform.get_feature_names_out(), data=X_test_tokens.toarray())\n",
    "test_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f13572f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.6564085245301869\n",
      "Val score: 0.6476634724977838\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(max_iter=5000, random_state=42, n_jobs=4)\n",
    "\n",
    "logreg.fit(train_vectors, y_train)\n",
    "\n",
    "print(f'Train score: {logreg.score(train_vectors, y_train)}')\n",
    "print(f'Val score: {logreg.score(val_vectors, y_val)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f102d1-8f58-4f83-82ae-bad91d86d68d",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc16a479-ae22-4eba-b87d-e0e8531eaa59",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Model performance was within the range of previous logreg models. As discussed previously, `CountVectorizer` and `TFIDF` have their limitations, and added context will not necessarily aid in their performance. Splitting the `max_features` between the two is also not ideal as it can hinder the predictability of the more immediate predictor, which is the `comment`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2545b1be-47fd-4893-b057-065ee49d2b98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
