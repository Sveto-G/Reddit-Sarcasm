{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06361c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gravi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "pd.set_option('display.max_columns', None, 'max_colwidth', 250)\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88e2b71f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NC and NH.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>You do know west teams play against west teams more than east teams right?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>They were underdogs earlier today, but since Gronk's announcement this afternoon, the Vegas line has moved to patriots -1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>This meme isn't funny none of the \"new york nigga\" ones are.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>I could use one of those tools.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  \\\n",
       "0      0   \n",
       "1      0   \n",
       "2      0   \n",
       "3      0   \n",
       "4      0   \n",
       "\n",
       "                                                                                                                     comment  \n",
       "0                                                                                                                 NC and NH.  \n",
       "1                                                 You do know west teams play against west teams more than east teams right?  \n",
       "2  They were underdogs earlier today, but since Gronk's announcement this afternoon, the Vegas line has moved to patriots -1  \n",
       "3                                                               This meme isn't funny none of the \"new york nigga\" ones are.  \n",
       "4                                                                                            I could use one of those tools.  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit = pd.read_csv('reddit_final.csv', index_col=[0])\n",
    "\n",
    "reddit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e1ce1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = reddit['comment']\n",
    "y = reddit['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27a6ed47",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_rem, X_test, y_rem, y_test = train_test_split(X, y, test_size = 0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "003758ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# second split into train and validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_rem, y_rem, test_size = 0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbac0450",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.PorterStemmer()\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def tokenizer(sentence):\n",
    "    \n",
    "    num_range = range(0,31)\n",
    "    \n",
    "    for num in num_range:\n",
    "        sentence = sentence.replace(str(num), '')\n",
    "    \n",
    "    # remove punctuation and set to lower case\n",
    "    for punctuation_mark in string.punctuation:\n",
    "        sentence = sentence.replace(punctuation_mark,'').lower()\n",
    "        \n",
    "    \n",
    "    # split sentence into words\n",
    "    words_list = sentence.split(' ')\n",
    "    tokens = []\n",
    "    \n",
    "    \n",
    "    # remove stopwords and any tokens that are just empty strings\n",
    "    for word in words_list:\n",
    "        if (not word in stop_words) and (word!=''):\n",
    "            # Stem words\n",
    "            stemmed_word = stemmer.stem(word)\n",
    "            tokens.append(stemmed_word)\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3b1104",
   "metadata": {},
   "source": [
    "change the x-train in here because we will vectorize the shit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785767bd",
   "metadata": {},
   "source": [
    "## metric absolutely needs to be 'cosine' Other distances make no sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87ccb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_model = KNeighborsClassifier(n_neighbors=3)\n",
    "KNN_model.fit(X_train, y_train)\n",
    "\n",
    "# Score the model on the test set\n",
    "test_predictions = KNN_model.predict(X_test)\n",
    "test_accuracy = accuracy_score(test_predictions, y_test)\n",
    "print(f\"Test set accuracy: {test_accuracy}\")\n",
    "\n",
    "PlotBoundaries(KNN_model, X, y, plotsize=(10,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e88748",
   "metadata": {},
   "source": [
    "Remember to add cross-validation in here because we don't have a val set. But realistically this will take for-freaking-ever.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af89ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors = range(1, X_train.shape[0], 2)  # step by 2 so only odd numbers show up\n",
    "\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "\n",
    "for n in neighbors: \n",
    "    print(f\"Working on my model with {n} neighbors...\", end=\"\\r\")\n",
    "    \n",
    "    #Instantiate the model & fit it to our data\n",
    "    KNN_model = KNeighborsClassifier(n_neighbors=n)\n",
    "    KNN_model.fit(X_train, y_train)\n",
    "    \n",
    "    #Score the model on the test set\n",
    "    train_accuracy = KNN_model.score(X_train, y_train)\n",
    "    test_accuracy = KNN_model.score(X_test, y_test)\n",
    "    \n",
    "    train_acc.append(train_accuracy)\n",
    "    test_acc.append(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4129c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trick to calculate the ideal k value: \n",
    "\n",
    "# index the value that is largest in the test accuracy\n",
    "index_of_max = np.argmax(test_acc)\n",
    "\n",
    "# the corresponding best k value\n",
    "best_k = neighbors[index_of_max]\n",
    "\n",
    "best_k"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
