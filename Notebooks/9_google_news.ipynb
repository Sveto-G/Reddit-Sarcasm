{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12b91057-bbdb-463e-88cc-309bbbc11615",
   "metadata": {},
   "source": [
    "### Notebook #9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c0e6f2-893c-4c00-84af-08f5d240db77",
   "metadata": {},
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358d6d85-46d9-4cbd-90c4-e3efd89a28bd",
   "metadata": {},
   "source": [
    "As a final foray into the land of embeddings, we will examine the `Word2Vec Google News` embeddings' ability to predict sarcasm. The rationale behind this decision is that it has been trained on an enormous amount of data and possesses 3 million tokens, which may help it better identify relationships between sarcastic comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f7cf550-848a-488f-a10d-8a122887bcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import regex as re\n",
    "import string\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(action = 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "721c80c6-0e59-490d-bb0a-f6016ff2954c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NC and NH.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>You do know west teams play against west teams...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>They were underdogs earlier today, but since G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>This meme isn't funny none of the \"new york ni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>I could use one of those tools.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment\n",
       "0      0                                         NC and NH.\n",
       "1      0  You do know west teams play against west teams...\n",
       "2      0  They were underdogs earlier today, but since G...\n",
       "3      0  This meme isn't funny none of the \"new york ni...\n",
       "4      0                    I could use one of those tools."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading in csv and checking head\n",
    "reddit = pd.read_csv('reddit_comments.csv', index_col=0)\n",
    "\n",
    "reddit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b18f55c1-9f7b-41fc-8fa4-74f5c9c24371",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2381faf-cf59-418e-a379-caf9a0bada57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec Google News consists of 3000000 tokens and 300 dimensional vectors\n"
     ]
    }
   ],
   "source": [
    "shape = embeddings.vectors.shape\n",
    "\n",
    "print(f'Word2Vec Google News consists of {shape[0]} tokens and {shape[1]} dimensional vectors')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a882d0-86bf-4c7e-af47-e241225592c2",
   "metadata": {},
   "source": [
    "At 3 million tokens, these embeddings outnumber the self-trained embedding tokens by a factor of 60."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd27e451-db4f-47cd-b6e1-e421709281d1",
   "metadata": {},
   "source": [
    "We will again be performing a sort of quick 'EDA' on what these embeddings hold and the differences in their similarities compared to the self-trained embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29105f98-8d12-410d-bc9e-75131349c21b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('snark', 0.6535126566886902),\n",
       " ('humor', 0.6242319345474243),\n",
       " ('sarcastic', 0.6220389604568481),\n",
       " ('self_deprecating_humor', 0.6171834468841553),\n",
       " ('self_deprecation', 0.607752799987793),\n",
       " ('condescension', 0.6044954061508179),\n",
       " ('snarkiness', 0.5990619659423828),\n",
       " ('irony', 0.5971057415008545),\n",
       " ('facetiousness', 0.5904334783554077),\n",
       " ('wit', 0.5846673250198364)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.most_similar('sarcasm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5713d289-3fcd-4f43-98e8-2cfdfa4b2271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('genuinely', 0.5996334552764893),\n",
       " ('real', 0.5876179933547974),\n",
       " ('legitimate', 0.5874236822128296),\n",
       " ('sincere', 0.5649887323379517),\n",
       " ('bona_fide', 0.5497070550918579),\n",
       " ('geniune', 0.5368186235427856),\n",
       " ('thief_demagogue_liar', 0.5333326458930969),\n",
       " ('genuineness', 0.5200586318969727),\n",
       " ('legitimate_aand', 0.5138376951217651),\n",
       " ('Genuine', 0.5109951496124268)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.most_similar('genuine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d5af41d-3acb-44c5-b4e0-ff670f6fa007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('frank', 0.6541845202445984),\n",
       " ('truthful', 0.6527417898178101),\n",
       " ('brutally_honest', 0.6292597055435181),\n",
       " ('dignity_Aujali', 0.6224915981292725),\n",
       " ('gazillionaire_Carella', 0.6192165017127991),\n",
       " ('honestly', 0.6042546629905701),\n",
       " ('forthright', 0.603355348110199),\n",
       " ('honesty', 0.576174259185791),\n",
       " ('candid', 0.5652011632919312),\n",
       " ('scrupulously_honest', 0.5450152158737183)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.most_similar('honest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0021ca98-e1a9-4f44-aaac-61580b1abb76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mislead', 0.6863378286361694),\n",
       " ('inaccurate', 0.6856165528297424),\n",
       " ('grossly_misleading', 0.673831582069397),\n",
       " ('deliberately_misleading', 0.6662497520446777),\n",
       " ('misled', 0.6598524451255798),\n",
       " ('Misleading', 0.647882342338562),\n",
       " ('deceptive', 0.6444677710533142),\n",
       " ('misrepresented', 0.6297670602798462),\n",
       " ('misrepresenting', 0.628930926322937),\n",
       " ('false', 0.6191345453262329)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.most_similar('misleading')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c4fe09c-02e8-4ce0-bd00-9116372d9a88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>serious</th>\n",
       "      <th>joking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>serous</td>\n",
       "      <td>joked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>severe</td>\n",
       "      <td>cracking_jokes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Serious</td>\n",
       "      <td>kidding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>seriously</td>\n",
       "      <td>laughing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>minor</td>\n",
       "      <td>joke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>seri_ous</td>\n",
       "      <td>laughed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>seriousness</td>\n",
       "      <td>chuckling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gravest</td>\n",
       "      <td>jokingly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>grievous</td>\n",
       "      <td>laugh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>nonserious</td>\n",
       "      <td>chuckled</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       serious          joking\n",
       "0       serous           joked\n",
       "1       severe  cracking_jokes\n",
       "2      Serious         kidding\n",
       "3    seriously        laughing\n",
       "4        minor            joke\n",
       "5     seri_ous         laughed\n",
       "6  seriousness       chuckling\n",
       "7      gravest        jokingly\n",
       "8     grievous           laugh\n",
       "9   nonserious        chuckled"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similarity scores for 'serious' and 'joking'\n",
    "serious = embeddings.most_similar('serious')\n",
    "joking = embeddings.most_similar('joking')\n",
    "\n",
    "# setting to dataframe\n",
    "pd.DataFrame(\n",
    "        data={\"serious\": [word for word, sim in serious], \n",
    "            \"joking\": [word for word, sim in joking]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28a45523-18c1-4c16-8235-556ded8a0159",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lie</th>\n",
       "      <th>truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lies</td>\n",
       "      <td>truths</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lying</td>\n",
       "      <td>falsehood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lying</td>\n",
       "      <td>veritas_Latin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Terravista_complex</td>\n",
       "      <td>Fatma_Trad_veiled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lay</td>\n",
       "      <td>truthful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Lie</td>\n",
       "      <td>facts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>perjure_yourself</td>\n",
       "      <td>Truth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sit</td>\n",
       "      <td>untruths</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>BE_TRUTHFUL_Do</td>\n",
       "      <td>falsity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>lurk</td>\n",
       "      <td>unvarnished_truth</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  lie              truth\n",
       "0                lies             truths\n",
       "1               lying          falsehood\n",
       "2               Lying      veritas_Latin\n",
       "3  Terravista_complex  Fatma_Trad_veiled\n",
       "4                 lay           truthful\n",
       "5                 Lie              facts\n",
       "6    perjure_yourself              Truth\n",
       "7                 sit           untruths\n",
       "8      BE_TRUTHFUL_Do            falsity\n",
       "9                lurk  unvarnished_truth"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finding most similar words to 'truth' and 'lie'\n",
    "lie = embeddings.most_similar('lie')\n",
    "truth = embeddings.most_similar('truth')\n",
    "\n",
    "# setting to dataframe to visualize\n",
    "pd.DataFrame(\n",
    "        data={\"lie\": [word for word, sim in lie], \n",
    "            \"truth\": [word for word, sim in truth]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163e5b8f-89df-4861-8d92-65aece9b78b8",
   "metadata": {},
   "source": [
    "### Observations\n",
    "- even more repetition than glove-twitter as exemplified in the 'lie' table\n",
    "- interestingly, it incorporates completely different meanings of the word 'lie', like 'lay'(down), which the other embeddings did not do\n",
    "- there is definitely more messiness in these embeddings with entries like: 'Fatma_Trad_veiled', which are difficult to understand\n",
    "- most importantly, these embeddings have a completely different construction as they incorporate `bigrams`\n",
    "- upper and lower case tokens are treated as different from one another"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c10f7af-8edd-4c6b-a324-bd07f53b967c",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9790889a-c7fb-4947-a050-1559d4fb8a51",
   "metadata": {},
   "source": [
    "**Below we will run our comments through the tokenizer and vectorizer functions and perform a train/test split.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2e4568e-bdea-4502-bed5-1bb92028bad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom tokenizer function\n",
    "\n",
    "def tokenizer(document):\n",
    "\n",
    "    \n",
    "    #removing punctuation\n",
    "    for punc in string.punctuation:\n",
    "        document = document.replace(punc, '')\n",
    "        # removing numbers and setting all documents to lowercase    \n",
    "    document = re.sub(\"\\d+\", \"\", document).lower()\n",
    "        # splitting documents and appending tokens list\n",
    "    tokens = document.split(' ')\n",
    "        \n",
    "    \n",
    "        \n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfbacbef-da38-4c22-9f14-76aa1f8e06c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizer(document):\n",
    "\n",
    "    tokens = tokenizer(document) # calling tokenizer function\n",
    "    \n",
    "    size = embeddings.vector_size # setting constant document size\n",
    "    vec_doc = np.zeros(size) # populating list of document size with zeros \n",
    "    count = 1 # word count to be used for average\n",
    "    \n",
    "    # looping through documents\n",
    "    for word in tokens:\n",
    "        # checking if word exists in document\n",
    "        if word in embeddings:\n",
    "            count +=1\n",
    "            vec_doc += embeddings[word] # adding word embedding to doc embedding\n",
    "    \n",
    "    vec_doc = vec_doc / count # taking average of embeddings\n",
    "    \n",
    "    \n",
    "    return vec_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0155a1b6-45e5-46d6-b6dd-537d52cf0d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying vectorizer function\n",
    "reddit['vectors'] = reddit['comment'].apply(vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21164efd-8d2b-4f10-9d39-6053721bbeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating x and y\n",
    "X = reddit['vectors']\n",
    "y = reddit['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b099f1c-51cb-410c-8dda-86f7e62648a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = list(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd63ea6d-36df-47e6-866e-e5d4ef19d6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test split\n",
    "X_rem, X_test, y_rem, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_rem, y_rem, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0dd007-05b7-4532-a31c-12201082099e",
   "metadata": {},
   "source": [
    "Now we can fit our data to our MLP Classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "414f7d55-c0d3-47c1-bb15-a1ea7aa7b7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.63216251\n",
      "Validation score: 0.651645\n",
      "Iteration 2, loss = 0.61039095\n",
      "Validation score: 0.665770\n",
      "Iteration 3, loss = 0.60059742\n",
      "Validation score: 0.672594\n",
      "Iteration 4, loss = 0.59378932\n",
      "Validation score: 0.672401\n",
      "Iteration 5, loss = 0.58912228\n",
      "Validation score: 0.673861\n",
      "Iteration 6, loss = 0.58514505\n",
      "Validation score: 0.675690\n",
      "Iteration 7, loss = 0.58167183\n",
      "Validation score: 0.679331\n",
      "Iteration 8, loss = 0.57861030\n",
      "Validation score: 0.682128\n",
      "Iteration 9, loss = 0.57588682\n",
      "Validation score: 0.682761\n",
      "Iteration 10, loss = 0.57342540\n",
      "Validation score: 0.674265\n",
      "Iteration 11, loss = 0.57119940\n",
      "Validation score: 0.684027\n",
      "Iteration 12, loss = 0.56926821\n",
      "Validation score: 0.673403\n",
      "Iteration 13, loss = 0.56743470\n",
      "Validation score: 0.683939\n",
      "Iteration 14, loss = 0.56574554\n",
      "Validation score: 0.683869\n",
      "Iteration 15, loss = 0.56395627\n",
      "Validation score: 0.676780\n",
      "Iteration 16, loss = 0.56276350\n",
      "Validation score: 0.679718\n",
      "Iteration 17, loss = 0.56143808\n",
      "Validation score: 0.680597\n",
      "Iteration 18, loss = 0.56006004\n",
      "Validation score: 0.681899\n",
      "Iteration 19, loss = 0.55850884\n",
      "Validation score: 0.683711\n",
      "Iteration 20, loss = 0.55751622\n",
      "Validation score: 0.676851\n",
      "Iteration 21, loss = 0.55666741\n",
      "Validation score: 0.684467\n",
      "Iteration 22, loss = 0.55519779\n",
      "Validation score: 0.676833\n",
      "Iteration 23, loss = 0.55452396\n",
      "Validation score: 0.684344\n",
      "Iteration 24, loss = 0.55354001\n",
      "Validation score: 0.682497\n",
      "Iteration 25, loss = 0.55232443\n",
      "Validation score: 0.681530\n",
      "Iteration 26, loss = 0.55191925\n",
      "Validation score: 0.683359\n",
      "Iteration 27, loss = 0.55073812\n",
      "Validation score: 0.679999\n",
      "Iteration 28, loss = 0.54987621\n",
      "Validation score: 0.681072\n",
      "Iteration 29, loss = 0.54987347\n",
      "Validation score: 0.679067\n",
      "Iteration 30, loss = 0.54871993\n",
      "Validation score: 0.682268\n",
      "Iteration 31, loss = 0.54825346\n",
      "Validation score: 0.676974\n",
      "Iteration 32, loss = 0.54745292\n",
      "Validation score: 0.679700\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(early_stopping=True, max_iter=50, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(early_stopping=True, max_iter=50, verbose=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(early_stopping=True, max_iter=50, verbose=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "MLP = MLPClassifier(verbose=True, max_iter=50, early_stopping=True)\n",
    "\n",
    "MLP.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ee5e1d7-9d81-474f-803e-1d4035b03c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.7145706616759832 \n",
      " Val Score: 0.683476774190144\n"
     ]
    }
   ],
   "source": [
    "print(f'Train Score: {MLP.score(X_train, y_train)} \\n Val Score: {MLP.score(X_val, y_val)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63b3f403-4b97-4e2c-9ef9-02efb8847947",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = MLP.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e56faf6-c25d-459a-8717-7b727da9445b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non-Sarcastic       0.68      0.70      0.69     94698\n",
      "    Sarcastic       0.69      0.67      0.68     94811\n",
      "\n",
      "     accuracy                           0.68    189509\n",
      "    macro avg       0.68      0.68      0.68    189509\n",
      " weighted avg       0.68      0.68      0.68    189509\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(y_val, predictions, target_names=['Non-Sarcastic', 'Sarcastic'])\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4139eada-f171-4afb-90d7-d729977c8452",
   "metadata": {},
   "source": [
    "**Again, we see similar performance to the other two embeddings, but more in line with the `Glove-Twitter` embeddings than the self-trained.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fd3582-55db-4981-a037-752c19867869",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00a35e8-2e9e-44f1-8122-ec66e828b443",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff1fc01-5fe4-4960-be96-a26e524474f4",
   "metadata": {},
   "source": [
    "We ran `Logistic Regression`, `KNN` and `MLP` on our text data, and `MLP` had the highest performance. We would be remise not to mention though that the tokenization and vectorization methods we used for the former two models were different, in `CountVectorizer` and `TFIDF`. This process demonstrated how the quality of the data and the transformations one makes in feature engineering are paramount to a strong result. If we were to use any of the word embeddings explored in these notebooks on `Logistic Regression` or `KNN` they would undoubtedly perform better.  \n",
    "\n",
    "Interestingly, it was our self-trained embeddings that outperformed the pre-trained ones with both much less training data and a much lower vocabulary. An accuracy of 70% is far from perfect, but considering the complexity of the problem, it is a very welcome result."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sarcasm_v3",
   "language": "python",
   "name": "sarcasm_v3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
